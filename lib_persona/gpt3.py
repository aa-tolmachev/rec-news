import nltk
import time

from nltk.tokenize import sent_tokenize
import requests
from lib_cleaner import cleaner
from lib_formatter import formatter

import traceback
import os
from lib_log import simple_log
global module_name
module_name = os.path.basename(__file__) #module name file

# –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è
def get_few_sentences(sentences, max_len=100):
    sentences_amount = 0
    total_length = 0
    for sentence in sentences:
        if (total_length + len(sentence) >= max_len):
            break
        else:
            total_length += len(sentence)
            sentences_amount += 1
    result = " ".join(sentences[:sentences_amount])
    return result

# –¥–æ—Å—Ç–∞–µ–º –∫–æ–º–º–µ–Ω—Ç –∏–∑ –∑–∞—Ç—Ä–∞–≤–∫–∏ —á–µ—Ä–µ–∑ api gpt3
def get_comment(text):
    comment = ''
    url = 'https://api.aicloud.sbercloud.ru/public/v1/public_inference/gpt3/predict'
    start_word = '–ù–æ–≤–æ—Å—Ç—å: '
    end_word = '–Ø –¥—É–º–∞—é, —á—Ç–æ '

    seed = start_word + text + end_word

    params = {
        'text': seed
    }
    req = requests.post(url, json=params)
    print(req)
    try:
        res = req.json()
        pred = res['predictions']
        start_index = len(seed)
        tokens = sent_tokenize(pred[start_index:], language='russian')
        print(tokens)
        comment = get_few_sentences(tokens)
    except:
        traceback.print_exc()

    return comment

def get_summary_with_comment(clean_summary, formatted_summary):
    step = 0

    # –¥–µ–ª–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∫–æ–º–º–µ–Ω—Ç
    for _ in range(3):
        comment = get_comment(clean_summary)
        if comment:
            comment = cleaner.fresh_text(comment)
            comment = formatter.format_comment(comment)
            formatted_summary = formatted_summary + '\n\n' + 'üí¨ ' + comment
            step = simple_log.make_log('end', module_name, step, message=f'added gpt3 generation')
            break
        time.sleep(1.5)


    return formatted_summary



